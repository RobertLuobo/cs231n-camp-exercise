课程链接：https://github.com/sharedeeply/cs231n-camp
作业答案：https://github.com/sharedeeply/cs231n-assignment
作业答案：https://github.com/sharedeeply/cs231n-assignment/blob/master/assignment1/cs231n

桌面图标Git Bash
git remote add origin https://github.com/duanzhihua/cs231n-camp.git
git教程
    https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000
git操作
cd /d/PycharmProjects/gitCs231_2018/
git status
git add cs231n/classifiers/
git commit -m "commit k_nearest_neighbor"
git push -u origin master

2018.11.4
Week7
如何更好的训练网络(下)
slides: lecture07
观看视频 p16, p17 和 p18, 了解训练神经网络中更多的标准化方法以及更多的学习率更新策略
学习神经网络笔记3 上 结束
    -合理性检查的提示与技巧
 学习神经网络笔记3 下 继续
    https://zhuanlan.zhihu.com/p/21798784
作业:

完成 assignment2 中 PyTorch CIFAR10 PyTorch.ipynb
学习深度学习中各种优化算法的总结
打kaggle比赛 cifar10, 模板代码



2018.10.7
opencv 安装
G:\ProgramData\Anaconda3\Scripts>pip install opencv-python

2018.10.6 淘宝项目 购买kaggle
房价预测  ok
可以看看 ok
https://blog.csdn.net/iam_emily/article/details/79307373

https://www.kaggle.com/dgawlik/house-prices-eda/notebook
单因素方差分析 ok
https://www.taodabai.com/how/469164964.html
python数据探索
https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python/notebook

安装
https://blog.csdn.net/zz860890410/article/details/78682041


2018.10.10
学习卷积神经网络笔记 OK
https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit


作业:继续
完成 assignment2 中 FullyConnectedNets.ipynb 和 BatchNormalization.ipynb
思考一下卷积神经网络对比传统神经网络的优势在哪里？为什么更适合处理图像问题
了解和学习深度学习中的normalization方法
https://blog.csdn.net/u014114990/article/details/52290064?utm_source=blogxgwz4

BN计算图
https://blog.csdn.net/u014114990/article/details/52290064?utm_source=blogxgwz4
公式
https://blog.csdn.net/qunnie_yi/article/details/80128445
2018.10.15

2018.10.25
如何更好的训练网络(上)
slides: lecture06
观看视频 p14, p15，学习训练神经网络中的激活函数，初始化和正则化方法
学习神经网络笔记1和神经网络笔记2
作业:

完成 assignment2 中 Dropout.ipynb 和 ConvolutionNetworks.ipynb OK 2018.11.3
了解和学习 fast conv 和 fast pooling 是如何实现的
卷积网络学习
http://www.cnblogs.com/hezhiyao/p/8168815.html

2018.9.30
linear_classifier.py  101 行 自己写的 漏了 axis=1 准确率就只有10% 每个知识点细节都太重要了；加上就达到40%了
2018.10.1
slide 4 ok 学习反向传播算法的笔记 OK
反向传播算法的数学补充  OK  例子 OK
可选项：反向传播算法的博客 OK
assignment1 中的 softmax.ipynb
two_layer_net.ipynb
softmax的 求导  手推数学公式 继续
https://blog.csdn.net/qian99/article/details/78046329
https://blog.csdn.net/u014313009/article/details/51045303



https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013758410364457b9e3d821f4244beb0fd69c61a185ae0000
C:\Users\lenovo\.ssh

https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840202368c74be33fbd884e71b570f2cc3c0d1dcf000
PPT：D:\大数据临时视频tmp\cs231n_2017课件 (4)
2018.9.23 第一周作业完成
git操作
https://blog.csdn.net/Hanani_Jia/article/details/79855429
git电脑操作：
https://blog.csdn.net/Hanani_Jia/article/details/77950594

2018.9.24
1，学习线性分类器[中下], 损失函数和优化器 PPT
Nearest Neighbor分类器的
优点：
Nearest Neighbor分类器容易理解，实现也比较简单。
缺点：
Nearest Neighbor模型训练只将数据赋值给类变量，没消耗什么时间；而模型测试要消耗大量的时间，因为每一个测试的图像都要和所有的训练图像进行比较，找到和测试图片最相近的图片。但在实际应用中，我们希望的是模型训练花费很多时间，一旦训练完成，拿到训练模型以后，对于新的数据测试就非常快。
Nearest Neighbor分类器不适用于实际的图像分类。 
图像是高维度数据，将图片进行部分遮挡、移动、染色以后，可能KNN计算的L1、L2距离是一样的，区分不出不同的图片。图像更多的是按照背景和颜色被分类，而不是语义主体分身。

线性分类器（2个常用的分类器：SVM、Softmax）
优点：
与kNN分类器不同，线性分类器通过训练学习参数（如权重W，偏执b等），一旦训练完成，就可以丢弃训练数据，拿到训练好的模型参数，对于新的测试数据，与权重W进行一个矩阵乘法运算，预测非常快。
   
线性分类笔记（上）ok
https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit
CS231n课程笔记翻译：线性分类笔记（中） ok
https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit
CS231n课程笔记翻译：最优化笔记（上） ok
https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit
CS231n课程笔记翻译：最优化笔记（下） ok
https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit

第二周作业：
svm公式汇总推导，Li 惩罚项λw^2。 在汇总时计算
https://blog.csdn.net/tuzixini/article/details/78635776

dWyi=−xi 1 (∑j≠yi1(xi⋅Wj−xi⋅Wyi+1>0))+2λWyi



